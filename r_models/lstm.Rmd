---
title: "lstm"
author: "Daniel Gao"
date: "2024-11-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Front Matter
```{r}
library(nnet)
library(keras)
library(tensorflow)
```


### Data
```{r}
test = read.csv("./test_timeseries.csv/test_timeseries.csv")
train = read.csv("./train_timeseries.csv/train_timeseries.csv") 
val = read.csv("./validation_timeseries.csv/validation_timeseries.csv") 
soil = read.csv("./soil_data.csv")
```


```{r}
#Combine test, train, val - will be used later in the code block
Drought <-
  test %>%
  bind_rows(train) %>%
  bind_rows(val)

DroughtED <-
  Drought %>%
  filter(score != 0)

cat("Results from removing the null values\n", 
    "Before:", nrow(Drought), "\n",
    "After:", nrow(DroughtED),"\n",
    "Total Removed:", nrow(Drought) - nrow(DroughtED),"\n",
    "Percentage Removed:",1-nrow(DroughtED)/nrow(Drought) )
```
### Splitting Data 
```{r}
# Train/Validation split
set.seed(315)
trainInd <- sample(1:nrow(DroughtED), floor(0.85*nrow(DroughtED)))
set.seed(NULL)

train <- DroughtED[trainInd, ]
val <- DroughtED[-trainInd, ]

train$score <- as.integer(train$score)
val$score <- as.integer(val$score)

x_train <- as.matrix(train[,-1])
y_train <- as.matrix(train[,1])

x_val <- as.matrix(val[,-1])
y_val <- as.matrix(val[,1])

n_classes <- length(unique(y_train))
shuffle_ind <- sample(nrow(x_train))
x_train <- x_train[shuffle_ind, , drop = FALSE]
y_train <- y_train[shuffle_ind, , drop = FALSE]


y_train[y_train == -1] <- 0
y_val [y_val  == -1] <- 0

dim(x_train) <- c(dim(x_train), 1)
dim(x_val) <- c(dim(x_val), 1)
```

```{r}
transformer_encoder <- function(inputs,
                                head_size,
                                num_heads,
                                ff_dim,
                                dropout = 0) {
  # Attention and Normalization
  attention_layer <-
    layer_multi_head_attention(key_dim = head_size,
                               num_heads = num_heads,
                               dropout = dropout)
  
  n_features <- dim(inputs) %>% tail(1)
  
  x <- inputs %>%
    attention_layer(., .) %>%
    layer_dropout(dropout) %>%
    layer_layer_normalization(epsilon = 1e-6)
  
  res <- x + inputs
  
  # Feed Forward Part
  x <- res %>%
    layer_conv_1d(ff_dim, kernel_size = 1, activation = "relu") %>%
    layer_dropout(dropout) %>%
    layer_conv_1d(n_features, kernel_size = 1) %>%
    layer_layer_normalization(epsilon = 1e-6)
  
  # return output + residual
  x + res
}


build_model <- function(input_shape,
                        head_size,
                        num_heads,
                        ff_dim,
                        num_transformer_blocks,
                        mlp_units,
                        dropout = 0,
                        mlp_dropout = 0) {
  
  inputs <- layer_input(input_shape)
  
  x <- inputs
  for (i in 1:num_transformer_blocks) {
    x <- x %>%
      transformer_encoder(
        head_size = head_size,
        num_heads = num_heads,
        ff_dim = ff_dim,
        dropout = dropout
      )
  }
  
  x <- x %>% 
    layer_global_average_pooling_1d(data_format = "channels_first")
  
  for (dim in mlp_units) {
    x <- x %>%
      layer_dense(dim, activation = "relu") %>%
      layer_dropout(mlp_dropout)
  }
  
  outputs <- x %>% 
    layer_dense(n_classes, activation = "softmax")
  
  keras_model(inputs, outputs)
}
```

```{r}
input_shape <- dim(x_train)[-1] # drop batch dim
model <- build_model(
  input_shape,
  head_size = 256,
  num_heads = 4,
  ff_dim = 4,
  num_transformer_blocks = 4,
  mlp_units = c(128),
  mlp_dropout = 0.4,
  dropout = 0.25
)
model %>% compile(
  loss = "sparse_categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 1e-4),
  metrics = c("sparse_categorical_accuracy")
)

model
```






